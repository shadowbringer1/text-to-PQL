{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting openai\n",
      "  Downloading openai-1.84.0-py3-none-any.whl (725 kB)\n",
      "\u001b[K     |████████████████████████████████| 725 kB 682 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx<1,>=0.23.0\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 139 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pydantic<3,>=1.9.0\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
      "\u001b[K     |████████████████████████████████| 444 kB 155 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<5,>=4.11 in /Users/suchang/Library/Python/3.9/lib/python/site-packages (from openai) (4.14.0)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting tqdm>4\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 503 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting anyio<5,>=3.5.0\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 540 kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting sniffio\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting jiter<1,>=0.4.0\n",
      "  Downloading jiter-0.10.0-cp39-cp39-macosx_11_0_arm64.whl (312 kB)\n",
      "\u001b[K     |████████████████████████████████| 312 kB 428 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna>=2.8\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 530 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: exceptiongroup>=1.0.2 in /Users/suchang/Library/Python/3.9/lib/python/site-packages (from anyio<5,>=3.5.0->openai) (1.3.0)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 389 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 228 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting h11>=0.16\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Collecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp39-cp39-macosx_11_0_arm64.whl (1.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9 MB 196 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sniffio, idna, h11, certifi, typing-inspection, pydantic-core, httpcore, anyio, annotated-types, tqdm, pydantic, jiter, httpx, distro, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 certifi-2025.4.26 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 openai-1.84.0 pydantic-2.11.5 pydantic-core-2.33.2 sniffio-1.3.1 tqdm-4.67.1 typing-inspection-0.4.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 正在生成场景：software_PSI\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "software_PSI 场景成功生成 11 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "software_PSI 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "software_PSI 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "software_PSI 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "software_PSI 场景成功生成 20 条。\n",
      "✅ 场景 software_PSI 共保留 91 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：software_MPC\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "software_MPC 场景成功生成 11 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "software_MPC 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "software_MPC 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "software_MPC 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "software_MPC 场景成功生成 20 条。\n",
      "✅ 场景 software_MPC 共保留 91 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：software_PIR\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "software_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "software_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "software_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "software_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "software_PIR 场景成功生成 20 条。\n",
      "✅ 场景 software_PIR 共保留 100 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：hardware_PSI\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "[警告] hardware_PSI 场景 JSON 解析失败，已保存原始文本供手动处理。\n",
      "  ⚠️ 第 1 次生成失败，跳过。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "hardware_PSI 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "[警告] hardware_PSI 场景 JSON 解析失败，已保存原始文本供手动处理。\n",
      "  ⚠️ 第 3 次生成失败，跳过。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "hardware_PSI 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "hardware_PSI 场景成功生成 15 条。\n",
      "✅ 场景 hardware_PSI 共保留 55 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：hardware_MPC\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "hardware_MPC 场景成功生成 9 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "hardware_MPC 场景成功生成 11 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "hardware_MPC 场景成功生成 10 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "[警告] hardware_MPC 场景 JSON 解析失败，已保存原始文本供手动处理。\n",
      "  ⚠️ 第 4 次生成失败，跳过。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "hardware_MPC 场景成功生成 12 条。\n",
      "✅ 场景 hardware_MPC 共保留 42 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：hardware_PIR\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "[警告] hardware_PIR 场景 JSON 解析失败，已保存原始文本供手动处理。\n",
      "  ⚠️ 第 1 次生成失败，跳过。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "hardware_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "hardware_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "hardware_PIR 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "hardware_PIR 场景成功生成 20 条。\n",
      "✅ 场景 hardware_PIR 共保留 80 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：hardware_PIRMPC\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "hardware_PIRMPC 场景成功生成 20 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "hardware_PIRMPC 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "hardware_PIRMPC 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "hardware_PIRMPC 场景成功生成 20 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "hardware_PIRMPC 场景成功生成 20 条。\n",
      "✅ 场景 hardware_PIRMPC 共保留 100 条去重后样本。\n",
      "\n",
      "🎯 正在生成场景：Federated_learning\n",
      "  ⏳ 第 1/5 次生成中...\n",
      "Federated_learning 场景成功生成 20 条。\n",
      "  ⏳ 第 2/5 次生成中...\n",
      "Federated_learning 场景成功生成 20 条。\n",
      "  ⏳ 第 3/5 次生成中...\n",
      "Federated_learning 场景成功生成 20 条。\n",
      "  ⏳ 第 4/5 次生成中...\n",
      "Federated_learning 场景成功生成 15 条。\n",
      "  ⏳ 第 5/5 次生成中...\n",
      "[警告] Federated_learning 场景 JSON 解析失败，已保存原始文本供手动处理。\n",
      "  ⚠️ 第 5 次生成失败，跳过。\n",
      "✅ 场景 Federated_learning 共保留 75 条去重后样本。\n",
      "\n",
      "✅ 全部生成完成，已保存至 pql_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-bCeqVHwJt0yXU5VIarZPidGnjGK510UKA19Dqh6EuASQrCEG\",\n",
    "    base_url=\"https://lonlie.plus7.plus/v1\"\n",
    ")\n",
    "\n",
    "scene_name_map = {\n",
    "    \"software_PSI\": \"软件PSI\",\n",
    "    \"software_MPC\": \"软件MPC\",\n",
    "    \"software_PIR\": \"软件PIR\",\n",
    "    \"hardware_PSI\": \"硬件PSI\",\n",
    "    \"hardware_MPC\": \"硬件MPC\",\n",
    "    \"hardware_PIR\": \"硬件PIR\",\n",
    "    \"hardware_PIRMPC\": \"硬件PIRMPC\",\n",
    "    \"Federated_learning\": \"联邦学习\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def build_prompt(rule, num_samples=100, previous_first_sample=None):\n",
    "    prompt = f\"\"\"\n",
    "You are a PQL generation assistant. Your task is to generate exactly {num_samples} high-quality question-PQL triples, formatted as a **raw JSON array** (no markdown or explanatory text), like this:\n",
    "\n",
    "[\n",
    "  {{\n",
    "    \"question\": \"...\",\n",
    "    \"Chinese_question\": \"...\",\n",
    "    \"PQL_query\": \"...\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "## Generation Requirements:\n",
    "1. Each triple must describe a meaningful secure computation task.\n",
    "2. All table names, field names, and tenant/platform names used in the PQL must appear **explicitly** in both the English and Chinese questions.\n",
    "3. English and Chinese questions must be **natural and fluent**, with **non-templated** phrasing. The Chinese question should NOT be a direct translation.\n",
    "4. Ensure **diversity** in question intent and structure — avoid duplicate or near-duplicate questions.\n",
    "5. If the scenario is a hardware environment (such as TEE), please specify it in the question.\n",
    "6. **Only return the JSON array.** Do not include any explanations, headings, markdown (such as ```), or extra text.\n",
    "\"\"\"\n",
    "    if previous_first_sample:\n",
    "        prompt += f\"\"\"\n",
    "\n",
    "## Constraint:\n",
    "Avoid generating samples that are similar in content, structure, or intent to the following existing one:\n",
    "\n",
    "{json.dumps(previous_first_sample, ensure_ascii=False, indent=2)}\n",
    "\"\"\"\n",
    "    prompt += f\"\"\"\n",
    "\n",
    "Based on the rule below, generate {num_samples} diverse and correct samples.\n",
    "\n",
    "Rule:\n",
    "{rule}\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_100_triplets(rule, scene, num_samples=100, previous_first_sample=None):\n",
    "    prompt_system = \"You are a database expert proficient in SQL and PQL for privacy-preserving applications.\"\n",
    "    user_prompt = build_prompt(rule, num_samples=num_samples, previous_first_sample=previous_first_sample)\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": prompt_system},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=1.0,\n",
    "            max_tokens=8192\n",
    "        )\n",
    "        content = completion.choices[0].message.content\n",
    "        content = unicodedata.normalize('NFKC', content)\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(content)\n",
    "            if isinstance(parsed, list):\n",
    "                print(f\"{scene} 场景成功生成 {len(parsed)} 条。\")\n",
    "                return parsed\n",
    "            else:\n",
    "                raise ValueError(\"返回内容不是 JSON 数组\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"[警告] {scene} 场景 JSON 解析失败，已保存原始文本供手动处理。\")\n",
    "            os.makedirs(\"fallback\", exist_ok=True)\n",
    "            with open(f\"fallback/{scene}_raw.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(content)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"[错误] {scene} 场景生成失败: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    rule_file = \"rules.json\"            # 输入规则文件，格式：{ \"场景名\": \"规则内容\", ... }\n",
    "    output_file = \"pql_dataset.json\"    # 最终输出文件\n",
    "\n",
    "    with open(rule_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        rules_dict = json.load(f)\n",
    "\n",
    "    final_data = {}\n",
    "\n",
    "    for scene, rule in rules_dict.items():\n",
    "        print(f\"\\n🎯 正在生成场景：{scene}\")\n",
    "        all_triplets = []\n",
    "        prev_first_sample = None\n",
    "\n",
    "        for i in range(5):\n",
    "            print(f\"  ⏳ 第 {i + 1}/5 次生成中...\")\n",
    "            triplets = generate_100_triplets(rule, scene, num_samples=20, previous_first_sample=prev_first_sample)\n",
    "            if triplets:\n",
    "                all_triplets.extend(triplets)\n",
    "                prev_first_sample = triplets[0]\n",
    "            else:\n",
    "                print(f\"  ⚠️ 第 {i + 1} 次生成失败，跳过。\")\n",
    "            time.sleep(1)\n",
    "\n",
    "        # 去重（以 question 为主）\n",
    "        unique_triplets = {}\n",
    "        for item in all_triplets:\n",
    "            q = item[\"question\"]\n",
    "            if q not in unique_triplets:\n",
    "                unique_triplets[q] = item\n",
    "        deduped = list(unique_triplets.values())[:100]\n",
    "\n",
    "        final_data[scene] = deduped\n",
    "        print(f\"✅ 场景 {scene} 共保留 {len(deduped)} 条去重后样本。\")\n",
    "\n",
    "    # 保存最终数据集\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n✅ 全部生成完成，已保存至 {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
